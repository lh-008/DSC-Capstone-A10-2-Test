method: bayesian
metric:
  name: loss #dpo loss, might need to change to reward func
  goal: minimize

parameters:
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3

  beta:
    distribution: log_uniform_values
    min: 0.01
    max: 0.3

  temperature: #might not sweep over, not related to DPO strictly
    distribution: log_uniform_values
    min: 0.5
    max: 1.5

  top_p: #might not sweep over, not related to DPO strictly
    distribution: log_uniform_values
    min: 0.8
    max: 0.95

  alpha0:
    distribution: log_uniform_values
    min: 0.05
    max: 0.3

  score_gap_min:
    distribution: log_uniform_values
    min: 0.01
    max: 0.3

  grad_accum:
    values: [2, 4, 8]