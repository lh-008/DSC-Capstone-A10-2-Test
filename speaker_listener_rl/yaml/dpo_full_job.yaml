apiVersion: batch/v1
kind: Job
metadata:
  name: dpo-full-training
  namespace: dsc-capstone-25-26
spec:
  backoffLimit: 0
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10

      initContainers:
      - name: init-permissions
        image: busybox
        command: ["sh", "-c", "mkdir -p /workspace/checkpoints && chmod -R 777 /workspace"]
        volumeMounts:
        - mountPath: /workspace
          name: storage-volume

      containers:
      - name: dpo-trainer
        image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
        workingDir: /workspace
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
            apt-get update && apt-get install -y git
            
            # Clean up if old repo exists and clone the repository
            rm -rf /workspace/repo  
            git clone https://github.com/banffjiang/DSC-Capstone-A10-2.git /workspace/repo
            cd /workspace/repo
            
            pip install --no-cache-dir -r requirements.txt
            
            wandb login $WANDB_API_KEY
            
            python speaker_listener_rl/training/dpo_full_train.py \
              --wandb_project dpo-training \
              --wandb_run_name simplewiki-run-$(date +%Y%m%d-%H%M) \
              --policy_model gpt2 \
              --input_path speaker_listener_rl/data/simple_wiki_passages_8k.jsonl \
              --output_path /workspace/checkpoints \
              --epochs 5 \
              --batch_size 16 \
              --grad_accum 8 \
              --lr 5e-6 \
              --alpha 0 \
              --beta 0.02 \
              --max_length 512 \
              --max_new_tokens 32 \
              --top_p 0.9 \
              --top_k 50 \
              --temperature 0.8 \
              --repetition_penalty 1.2 \
              --no_repeat_ngram_size 3 \
              --score_gap_min 0.01 \
              --max_pair_similarity 0.7 \
              --max_resample_tries 8 \
              --listener_model_type roberta-large \
              --listener_batch_size 8 \
              --run_validation \
              --validation_max_examples 128
            
            echo "checkpoints saved to /workspace/checkpoints"
            ls -lh /workspace/checkpoints/
        
        env:
        - name: WANDB_API_KEY
          value: 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
        - name: WANDB_PROJECT
          value: "dpo-training"
        - name: WANDB_MODE
          value: "online"
        - name: WORLD_SIZE
          value: "1"
        - name: RANK
          value: "0"
        - name: LOCAL_RANK
          value: "0"
        # - name: CUDA_LAUNCH_BLOCKING
        #   value: "1"
        - name: TORCH_SHOW_CPP_STACKTRACES
          value: "1"
        - name: HF_HOME
          value: /workspace/.cache/huggingface
        - name: HF_DATASETS_CACHE
          value: /workspace/.cache/huggingface/datasets
        - name: TRANSFORMERS_CACHE
          value: /workspace/.cache/huggingface/transformers
        - name: WANDB_DIR
          value: /workspace/wandb
        - name: PIP_CACHE_DIR
          value: /workspace/.cache/pip
        
        volumeMounts:
        - mountPath: /workspace
          name: storage-volume
        
        resources:
          requests:
            memory: "32Gi"
            cpu: "8"
            ephemeral-storage: "10Gi"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "16"
            ephemeral-storage: "20Gi"
            nvidia.com/gpu: "1"

      restartPolicy: Never

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: nautilus.io/issue
        operator: Exists
        effect: NoSchedule

      volumes:
      - name: storage-volume
        persistentVolumeClaim:
          claimName: dpo-full-storage
